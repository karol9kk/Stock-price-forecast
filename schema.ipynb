{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 10",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[0;32m     23\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 24\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m create_sequences(test_data, sequence_length)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Build the LSTM model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [4], line 20\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(data, sequence_length)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m sequence_length):\n\u001b[0;32m     19\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(data[i:i\u001b[38;5;241m+\u001b[39msequence_length])\n\u001b[1;32m---> 20\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3804\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3806\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Load and preprocess the time series data\n",
    "data = pd.read_csv('TSLA.csv')\n",
    "# Perform data preprocessing steps (e.g., handle missing values, normalization)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_data, test_data = data[:train_size], data[train_size:]\n",
    "\n",
    "# Prepare the input and output sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(data[i+sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 10\n",
    "X_train, y_train = create_sequences(train_data, sequence_length)\n",
    "X_test, y_test = create_sequences(test_data, sequence_length)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=(sequence_length, 1)))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = np.mean((predictions - y_test) ** 2)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dane początkowe\n",
    "data = daily_visits.copy()\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import  LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#Dane do treningu modelu\n",
    "df = data\n",
    "y = data['No_visits']\n",
    "y = y.values.reshape(-1, 1)\n",
    "\n",
    "# skalowanie danych\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(y)\n",
    "y = scaler.transform(y)\n",
    "\n",
    "\n",
    "n_lookback = 150  # dane na których będzie się uczył model\n",
    "n_forecast = 30  # długość prognozy\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(n_lookback, len(y) - n_forecast + 1):\n",
    "    X.append(y[i - n_lookback: i])\n",
    "    Y.append(y[i: i + n_forecast])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# budowa modelu\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=32, return_sequences=True, input_shape=(n_lookback, 1)))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dense(n_forecast))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, Y, epochs=100, batch_size=8, verbose=0)\n",
    "\n",
    "#prognozowanie\n",
    "X_ = y[- n_lookback:]  \n",
    "X_ = X_.reshape(1, n_lookback, 1)\n",
    "\n",
    "Y_ = model.predict(X_).reshape(-1, 1)\n",
    "Y_ = scaler.inverse_transform(Y_)\n",
    "\n",
    "# zapis wyników\n",
    "df_past = df[['No_visits']].reset_index()\n",
    "df_past.rename(columns={'index': 'Date', 'No_visits': 'Actual'}, inplace=True)\n",
    "df_past['Date'] = pd.to_datetime(df_past['Date'])\n",
    "df_past['Forecast'] = np.nan\n",
    "df_past['Forecast'].iloc[-1] = df_past['Actual'].iloc[-1]\n",
    "\n",
    "df_future = pd.DataFrame(columns=['Date', 'Actual', 'Forecast'])\n",
    "df_future['Date'] = pd.date_range(start=df_past['Date'].iloc[-1] + pd.Timedelta(days=1), periods=n_forecast)\n",
    "df_future['Forecast'] = Y_.flatten()\n",
    "df_future['Actual'] = np.nan\n",
    "\n",
    "results = df_past.append(df_future).set_index('Date')\n",
    "\n",
    "# wykres\n",
    "results.plot(title='Prognoza wizyt w lipcu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sequential_Input_LSTM(df, input_sequence):\n",
    "    df_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(df_np) - input_sequence):\n",
    "        row = [a for a in df_np[i:i + input_sequence]]\n",
    "        X.append(row)\n",
    "        label = df_np[i + input_sequence]\n",
    "        y.append(label)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "n_input = 10      \n",
    "\n",
    "df_min_model_data = df_hour_lvl['T']\n",
    "\n",
    "X, y = Sequential_Input_LSTM(df_min_model_data, n_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train, y_train = X[:60000], y[:60000]\n",
    "\n",
    "# Validation data\n",
    "X_val, y_val = X[60000:65000], y[60000:65000]\n",
    "\n",
    "# Test data\n",
    "X_test, y_test = X[65000:], y[65000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1                        \n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(InputLayer((n_input,n_features)))\n",
    "model1.add(LSTM(100, return_sequences = True))     \n",
    "model1.add(LSTM(100, return_sequences = True))\n",
    "model1.add(LSTM(50))\n",
    "model1.add(Dense(8, activation = 'relu'))\n",
    "model1.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_df1 = pd.DataFrame(model1.history.history)\n",
    "\n",
    "losses_df1.plot(figsize = (10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "save_model(model1, \"LSTM_Models/lstm_univariate.h5\")\n",
    "\n",
    "# load the model\n",
    "model1 = load_model('LSTM_Models/lstm_univariate.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions1 = model1.predict(X_test).flatten()\n",
    "\n",
    "\n",
    "X_test_list = []\n",
    "for i in range(len(X_test)):\n",
    "    X_test_list.append(X_test[i][0])\n",
    "    \n",
    "\n",
    "test_predictions_df1 = pd.DataFrame({'X_test':list(X_test_list), \n",
    "                                    'LSTM Prediction':list(test_predictions1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df1.plot(figsize = (15,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df1[(len(X_test) - 720):].plot(figsize = (15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def futureForecast(df, col, n_input, n_features, forecast_timeperiod, model):\n",
    "\n",
    "    x_input = np.array(df[len(df)-n_input:][col])\n",
    "\n",
    "    temp_input=list(x_input)\n",
    "\n",
    "    lst_output=[]\n",
    "    i=0\n",
    "\n",
    "    while(i < forecast_timeperiod):\n",
    "\n",
    "        if(len(temp_input) > n_input):\n",
    "\n",
    "            x_input = np.array(temp_input[1:])\n",
    "            x_input = x_input.reshape((1, n_input, n_features))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            temp_input.append(yhat[0][0])\n",
    "            temp_input = temp_input[1:]\n",
    "            lst_output.append(yhat[0][0])\n",
    "\n",
    "            i=i+1\n",
    "\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_input, n_features))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            #print(yhat[0])\n",
    "            temp_input.append(yhat[0][0])\n",
    "            lst_output.append(yhat[0][0])\n",
    "\n",
    "            i=i+1\n",
    "            \n",
    "    return lst_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 10\n",
    "n_features = 1\n",
    "forecast_timeperiod = 240         # next 10 days\n",
    "model = model1\n",
    "\n",
    "forecast_output = futureForecast(df, \n",
    "                                 'T', \n",
    "                                 n_input, \n",
    "                                 n_features, \n",
    "                                 forecast_timeperiod, \n",
    "                                 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_10_days = df['T'][len(df) - 240:].tolist()\n",
    "\n",
    "next_10_days = pd.DataFrame(forecast_output, columns = ['FutureForecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "hist_axis = len(last_10_days)\n",
    "forecast_axis = hist_axis + len(next_10_days)\n",
    "\n",
    "plt.plot(np.arange(0,hist_axis),last_10_days, color = 'blue')\n",
    "plt.plot(np.arange(hist_axis,forecast_axis),next_10_days['FutureForecast'].tolist(), color = 'orange')\n",
    "\n",
    "plt.title('LSTM Forecast for Next 10 Days')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Temperature')\n",
    "\n",
    "# save the figure\n",
    "plt.savefig('Pics_Models/lstm_univariate_forecast_pic.png')\n",
    "plt.savefig('Pics_Models/lstm_univariate_forecast_pdf.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(n_forecast))\n\u001b[0;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Forecasting\u001b[39;00m\n\u001b[0;32m     51\u001b[0m X_ \u001b[38;5;241m=\u001b[39m data[feature_columns]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39mn_lookback:]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, n_lookback, \u001b[38;5;28mlen\u001b[39m(feature_columns))\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\karol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = pd.read_csv(\"TSLA.csv\")\n",
    "\n",
    "# Ensure Date is in datetime format and set it as index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Identify target column and feature columns\n",
    "target_column = 'Close'\n",
    "feature_columns = data.columns.tolist()\n",
    "feature_columns.remove(target_column)\n",
    "\n",
    "# Scaling features and target column\n",
    "scalers = {}\n",
    "for column in data.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[[column]] = scaler.fit_transform(data[[column]])\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# Parameters\n",
    "n_lookback = 150  # number of past days to look back\n",
    "n_forecast = 30   # number of days to forecast\n",
    "\n",
    "# Prepare training data\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(n_lookback, len(data) - n_forecast + 1):\n",
    "    X.append(data[feature_columns].iloc[i - n_lookback: i].values)\n",
    "    Y.append(data[target_column].iloc[i: i + n_forecast].values)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=32, return_sequences=True, input_shape=(n_lookback, X.shape[2])))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dense(n_forecast))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, Y, epochs=100, batch_size=8, verbose=0)\n",
    "\n",
    "# Forecasting\n",
    "X_ = data[feature_columns].iloc[-n_lookback:].values.reshape(1, n_lookback, len(feature_columns))\n",
    "Y_ = model.predict(X_).reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the forecasted data\n",
    "Y_ = scalers[target_column].inverse_transform(Y_)\n",
    "\n",
    "# Prepare results\n",
    "df_past = data[[target_column]].reset_index()\n",
    "df_past.rename(columns={target_column: 'Actual'}, inplace=True)\n",
    "df_past['Forecast'] = np.nan\n",
    "df_past['Forecast'].iloc[-1] = df_past['Actual'].iloc[-1]\n",
    "\n",
    "df_future = pd.DataFrame(columns=['Date', 'Actual', 'Forecast'])\n",
    "df_future['Date'] = pd.date_range(start=df_past['Date'].iloc[-1] + pd.Timedelta(days=1), periods=n_forecast)\n",
    "df_future['Forecast'] = Y_.flatten()\n",
    "df_future['Actual'] = np.nan\n",
    "\n",
    "results = df_past.append(df_future).set_index('Date')\n",
    "\n",
    "# Print results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 800\n",
      "Validation data length: 200\n",
      "X_train shape: (721, 50, 5)\n",
      "Y_train shape: (721, 30)\n",
      "X_valid shape: (121, 50, 5)\n",
      "Y_valid shape: (121, 30)\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 3s 164ms/step - loss: 0.0608 - val_loss: 0.4698\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0438 - val_loss: 0.3204\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0271 - val_loss: 0.1903\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.0150 - val_loss: 0.1163\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.0078 - val_loss: 0.0644\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0048 - val_loss: 0.0503\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0034 - val_loss: 0.0428\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0027 - val_loss: 0.0435\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0024 - val_loss: 0.0409\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0022 - val_loss: 0.0393\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0021 - val_loss: 0.0384\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0021 - val_loss: 0.0361\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0020 - val_loss: 0.0367\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0019 - val_loss: 0.0349\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0019 - val_loss: 0.0360\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0018 - val_loss: 0.0356\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0018 - val_loss: 0.0341\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0018 - val_loss: 0.0352\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0017 - val_loss: 0.0314\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0017 - val_loss: 0.0349\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0017 - val_loss: 0.0356\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0016 - val_loss: 0.0317\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0016 - val_loss: 0.0276\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0016 - val_loss: 0.0251\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0016 - val_loss: 0.0243\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.0017 - val_loss: 0.0462\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0016 - val_loss: 0.0296\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0015 - val_loss: 0.0337\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0015 - val_loss: 0.0362\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0014 - val_loss: 0.0310\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0014 - val_loss: 0.0335\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0013 - val_loss: 0.0341\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0012 - val_loss: 0.0285\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0011 - val_loss: 0.0405\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0013 - val_loss: 0.0355\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0013 - val_loss: 0.0350\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0012 - val_loss: 0.0333\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0011 - val_loss: 0.0254\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0011 - val_loss: 0.0316\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.0011 - val_loss: 0.0275\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0010 - val_loss: 0.0264\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0010 - val_loss: 0.0269\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0010 - val_loss: 0.0344\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 9.7835e-04 - val_loss: 0.0339\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 9.4708e-04 - val_loss: 0.0323\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 9.2782e-04 - val_loss: 0.0347\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 8.9712e-04 - val_loss: 0.0337\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 8.8687e-04 - val_loss: 0.0355\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 9.2007e-04 - val_loss: 0.0324\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 8.8482e-04 - val_loss: 0.0303\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 8.1557e-04 - val_loss: 0.0303\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 8.0210e-04 - val_loss: 0.0297\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 8.0496e-04 - val_loss: 0.0340\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 7.7759e-04 - val_loss: 0.0308\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 7.6921e-04 - val_loss: 0.0308\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 7.7613e-04 - val_loss: 0.0354\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 7.5755e-04 - val_loss: 0.0362\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 7.6030e-04 - val_loss: 0.0421\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 7.5649e-04 - val_loss: 0.0333\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 7.4332e-04 - val_loss: 0.0292\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 7.3635e-04 - val_loss: 0.0332\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 7.3666e-04 - val_loss: 0.0444\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 7.2565e-04 - val_loss: 0.0365\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.9857e-04 - val_loss: 0.0396\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 6.9267e-04 - val_loss: 0.0372\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 6.8514e-04 - val_loss: 0.0395\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 6.8057e-04 - val_loss: 0.0321\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 7.0771e-04 - val_loss: 0.0454\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 7.1213e-04 - val_loss: 0.0508\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 7.2449e-04 - val_loss: 0.0385\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.9169e-04 - val_loss: 0.0393\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 6.8678e-04 - val_loss: 0.0495\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 6.8404e-04 - val_loss: 0.0459\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 6.8496e-04 - val_loss: 0.0429\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 6.6066e-04 - val_loss: 0.0406\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.8485e-04 - val_loss: 0.0478\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.5606e-04 - val_loss: 0.0484\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 6.5062e-04 - val_loss: 0.0463\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.4022e-04 - val_loss: 0.0487\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.3142e-04 - val_loss: 0.0477\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.2708e-04 - val_loss: 0.0477\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.2496e-04 - val_loss: 0.0460\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 6.3264e-04 - val_loss: 0.0510\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 6.3798e-04 - val_loss: 0.0480\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 6.2886e-04 - val_loss: 0.0461\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 6.2156e-04 - val_loss: 0.0505\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 6.1306e-04 - val_loss: 0.0491\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 6.1800e-04 - val_loss: 0.0502\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 6.1051e-04 - val_loss: 0.0500\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 6.0448e-04 - val_loss: 0.0489\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 6.0125e-04 - val_loss: 0.0504\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 5.9556e-04 - val_loss: 0.0496\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 5.9738e-04 - val_loss: 0.0497\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 5.9343e-04 - val_loss: 0.0480\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 5.9766e-04 - val_loss: 0.0496\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 6.0553e-04 - val_loss: 0.0528\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 6.0251e-04 - val_loss: 0.0556\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.0995e-04 - val_loss: 0.0493\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 5.9825e-04 - val_loss: 0.0499\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 5.8825e-04 - val_loss: 0.0502\n",
      "4/4 [==============================] - 1s 7ms/step\n",
      "Validation RMSE: 267.53832369362317\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "              Actual    Forecast\n",
      "Date                            \n",
      "2018-04-06  0.020154         NaN\n",
      "2018-04-09  0.018539         NaN\n",
      "2018-04-10  0.021058         NaN\n",
      "2018-04-11  0.020427         NaN\n",
      "2018-04-12  0.019280         NaN\n",
      "...              ...         ...\n",
      "2022-04-19       NaN  655.578430\n",
      "2022-04-20       NaN  691.111206\n",
      "2022-04-21       NaN  675.638611\n",
      "2022-04-22       NaN  655.669067\n",
      "2022-04-23       NaN  674.155029\n",
      "\n",
      "[1030 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_1180\\904146354.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_past['Forecast'].iloc[-1] = df_past['Actual'].iloc[-1]\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_1180\\904146354.py:108: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = df_past.append(df_future).set_index('Date')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Sample data\n",
    "data = pd.read_csv(\"TSLA.csv\")\n",
    "data = data.tail(1000)\n",
    "\n",
    "# Ensure Date is in datetime format and set it as index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Identify target column and feature columns\n",
    "target_column = 'Close'\n",
    "feature_columns = data.columns.tolist()\n",
    "feature_columns.remove(target_column)\n",
    "\n",
    "# Scaling features and target column\n",
    "scalers = {}\n",
    "for column in data.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[[column]] = scaler.fit_transform(data[[column]])\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# Parameters\n",
    "n_lookback = 50  # number of past days to look back\n",
    "n_forecast = 30   # number of days to forecast\n",
    "validation_split = 0.2\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_size = int(len(data) * (1 - validation_split))\n",
    "data_train = data.iloc[:train_size]\n",
    "data_valid = data.iloc[train_size:]\n",
    "\n",
    "print(f'Training data length: {len(data_train)}')\n",
    "print(f'Validation data length: {len(data_valid)}')\n",
    "\n",
    "# Prepare training data\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for i in range(n_lookback, len(data_train) - n_forecast + 1):\n",
    "    X_train.append(data_train[feature_columns].iloc[i - n_lookback: i].values)\n",
    "    Y_train.append(data_train[target_column].iloc[i: i + n_forecast].values)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "\n",
    "# Prepare validation data\n",
    "X_valid = []\n",
    "Y_valid = []\n",
    "\n",
    "for i in range(n_lookback, len(data_valid) - n_forecast + 1):\n",
    "    X_valid.append(data_valid[feature_columns].iloc[i - n_lookback: i].values)\n",
    "    Y_valid.append(data_valid[target_column].iloc[i: i + n_forecast].values)\n",
    "\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)\n",
    "\n",
    "print(f'X_valid shape: {X_valid.shape}')\n",
    "print(f'Y_valid shape: {Y_valid.shape}')\n",
    "\n",
    "# Check if the data arrays are not empty\n",
    "if X_train.size == 0 or Y_train.size == 0 or X_valid.size == 0 or Y_valid.size == 0:\n",
    "    raise ValueError(\"Expected input data to be non-empty.\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=32, return_sequences=True, input_shape=(n_lookback, X_train.shape[2])))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dense(n_forecast))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=128, verbose=1, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "Y_valid_pred = model.predict(X_valid)\n",
    "Y_valid_pred = scalers[target_column].inverse_transform(Y_valid_pred.reshape(-1, 1)).reshape(Y_valid_pred.shape)\n",
    "Y_valid_actual = scalers[target_column].inverse_transform(Y_valid.reshape(-1, 1)).reshape(Y_valid.shape)\n",
    "\n",
    "validation_rmse = np.sqrt(mean_squared_error(Y_valid_actual, Y_valid_pred))\n",
    "print(f'Validation RMSE: {validation_rmse}')\n",
    "\n",
    "# Forecasting using the whole dataset\n",
    "X_ = data[feature_columns].iloc[-n_lookback:].values.reshape(1, n_lookback, len(feature_columns))\n",
    "Y_ = model.predict(X_).reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the forecasted data\n",
    "Y_ = scalers[target_column].inverse_transform(Y_)\n",
    "\n",
    "# Prepare results\n",
    "df_past = data[[target_column]].reset_index()\n",
    "df_past.rename(columns={target_column: 'Actual'}, inplace=True)\n",
    "df_past['Forecast'] = np.nan\n",
    "df_past['Forecast'].iloc[-1] = df_past['Actual'].iloc[-1]\n",
    "\n",
    "df_future = pd.DataFrame(columns=['Date', 'Actual', 'Forecast'])\n",
    "df_future['Date'] = pd.date_range(start=df_past['Date'].iloc[-1] + pd.Timedelta(days=1), periods=n_forecast)\n",
    "df_future['Forecast'] = Y_.flatten()\n",
    "df_future['Actual'] = np.nan\n",
    "\n",
    "results = df_past.append(df_future).set_index('Date')\n",
    "\n",
    "# Print results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 800\n",
      "Validation data length: 200\n",
      "X_train shape: (721, 50, 5)\n",
      "Y_train shape: (721, 30)\n",
      "X_valid shape: (121, 50, 5)\n",
      "Y_valid shape: (121, 30)\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 3s 119ms/step - loss: 0.0616 - val_loss: 0.4857\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0471 - val_loss: 0.3482\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0321 - val_loss: 0.2217\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0183 - val_loss: 0.1377\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0098 - val_loss: 0.0758\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0056 - val_loss: 0.0537\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0036 - val_loss: 0.0451\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0029 - val_loss: 0.0446\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0026 - val_loss: 0.0434\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0024 - val_loss: 0.0414\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0022 - val_loss: 0.0392\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0022 - val_loss: 0.0403\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0021 - val_loss: 0.0402\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0020 - val_loss: 0.0426\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0020 - val_loss: 0.0399\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0020 - val_loss: 0.0411\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 0.0392\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0406\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0397\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 0.0394\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0019 - val_loss: 0.0391\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0385\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0018 - val_loss: 0.0406\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 0.0367\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0019 - val_loss: 0.0391\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0019 - val_loss: 0.0362\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0019 - val_loss: 0.0417\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0019 - val_loss: 0.0370\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0018 - val_loss: 0.0406\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0018 - val_loss: 0.0370\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0018 - val_loss: 0.0390\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0017 - val_loss: 0.0377\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0017 - val_loss: 0.0401\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0017 - val_loss: 0.0387\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0017 - val_loss: 0.0392\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0017 - val_loss: 0.0394\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0017 - val_loss: 0.0394\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0017 - val_loss: 0.0401\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0016 - val_loss: 0.0414\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0016 - val_loss: 0.0409\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0016 - val_loss: 0.0434\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0015 - val_loss: 0.0452\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0015 - val_loss: 0.0417\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.0015 - val_loss: 0.0487\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0014 - val_loss: 0.0514\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0013 - val_loss: 0.0494\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0012 - val_loss: 0.0455\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0012 - val_loss: 0.0600\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0011 - val_loss: 0.0593\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0011 - val_loss: 0.0496\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 9.8055e-04 - val_loss: 0.0564\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 9.5900e-04 - val_loss: 0.0614\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 8.9383e-04 - val_loss: 0.0540\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 8.7052e-04 - val_loss: 0.0608\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 8.2785e-04 - val_loss: 0.0585\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 8.0161e-04 - val_loss: 0.0587\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 8.0070e-04 - val_loss: 0.0589\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 8.3162e-04 - val_loss: 0.0613\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 8.5398e-04 - val_loss: 0.0574\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 8.4470e-04 - val_loss: 0.0563\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 8.2971e-04 - val_loss: 0.0633\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 7.5916e-04 - val_loss: 0.0603\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 7.4542e-04 - val_loss: 0.0596\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 7.4848e-04 - val_loss: 0.0627\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 7.4677e-04 - val_loss: 0.0608\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 7.0235e-04 - val_loss: 0.0614\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.9928e-04 - val_loss: 0.0629\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.9971e-04 - val_loss: 0.0622\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 6.8672e-04 - val_loss: 0.0612\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.9454e-04 - val_loss: 0.0631\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 6.9174e-04 - val_loss: 0.0608\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.7076e-04 - val_loss: 0.0635\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.6710e-04 - val_loss: 0.0611\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.7648e-04 - val_loss: 0.0614\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.6139e-04 - val_loss: 0.0619\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.8040e-04 - val_loss: 0.0635\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.7895e-04 - val_loss: 0.0600\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.5353e-04 - val_loss: 0.0647\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 6.3940e-04 - val_loss: 0.0620\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 6.5369e-04 - val_loss: 0.0592\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 6.4182e-04 - val_loss: 0.0642\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 6.3156e-04 - val_loss: 0.0622\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.2632e-04 - val_loss: 0.0641\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.1819e-04 - val_loss: 0.0638\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.1346e-04 - val_loss: 0.0635\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.1449e-04 - val_loss: 0.0620\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.2277e-04 - val_loss: 0.0635\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.2090e-04 - val_loss: 0.0668\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 6.1599e-04 - val_loss: 0.0639\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 6.0213e-04 - val_loss: 0.0639\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 5.9507e-04 - val_loss: 0.0655\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 5.9488e-04 - val_loss: 0.0635\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 5.9616e-04 - val_loss: 0.0664\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 6.1202e-04 - val_loss: 0.0662\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 5.9918e-04 - val_loss: 0.0639\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 5.8453e-04 - val_loss: 0.0650\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 5.7684e-04 - val_loss: 0.0650\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 5.7394e-04 - val_loss: 0.0650\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 5.8319e-04 - val_loss: 0.0684\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 5.8153e-04 - val_loss: 0.0653\n",
      "4/4 [==============================] - 1s 7ms/step\n",
      "Validation RMSE: 305.08463897298566\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "              Actual    Forecast\n",
      "Date                            \n",
      "2018-04-06  0.020154         NaN\n",
      "2018-04-09  0.018539         NaN\n",
      "2018-04-10  0.021058         NaN\n",
      "2018-04-11  0.020427         NaN\n",
      "2018-04-12  0.019280         NaN\n",
      "...              ...         ...\n",
      "2022-04-19       NaN  690.661804\n",
      "2022-04-20       NaN  697.023376\n",
      "2022-04-21       NaN  672.002136\n",
      "2022-04-22       NaN  679.508972\n",
      "2022-04-23       NaN  681.149780\n",
      "\n",
      "[1030 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_1180\\2368592516.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_past['Forecast'].iloc[-1] = df_past['Actual'].iloc[-1]\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_1180\\2368592516.py:113: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = df_past.append(df_future).set_index('Date')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and prepare the data\n",
    "data = pd.read_csv(\"TSLA.csv\")\n",
    "data = data.tail(1000)\n",
    "\n",
    "# Ensure Date is in datetime format and set it as index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Identify target column and feature columns\n",
    "target_column = 'Close'\n",
    "feature_columns = data.columns.tolist()\n",
    "feature_columns.remove(target_column)\n",
    "\n",
    "# Scaling features and target column\n",
    "scalers = {}\n",
    "for column in data.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[[column]] = scaler.fit_transform(data[[column]])\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# Parameters\n",
    "n_lookback = 50  # number of past days to look back\n",
    "n_forecast = 30   # number of days to forecast\n",
    "validation_split = 0.2\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_size = int(len(data) * (1 - validation_split))\n",
    "data_train = data.iloc[:train_size]\n",
    "data_valid = data.iloc[train_size:]\n",
    "\n",
    "print(f'Training data length: {len(data_train)}')\n",
    "print(f'Validation data length: {len(data_valid)}')\n",
    "\n",
    "# Prepare training data\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for i in range(n_lookback, len(data_train) - n_forecast + 1):\n",
    "    X_train.append(data_train[feature_columns].iloc[i - n_lookback: i].values)\n",
    "    Y_train.append(data_train[target_column].iloc[i: i + n_forecast].values)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "\n",
    "# Prepare validation data\n",
    "X_valid = []\n",
    "Y_valid = []\n",
    "\n",
    "for i in range(n_lookback, len(data_valid) - n_forecast + 1):\n",
    "    X_valid.append(data_valid[feature_columns].iloc[i - n_lookback: i].values)\n",
    "    Y_valid.append(data_valid[target_column].iloc[i: i + n_forecast].values)\n",
    "\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)\n",
    "\n",
    "print(f'X_valid shape: {X_valid.shape}')\n",
    "print(f'Y_valid shape: {Y_valid.shape}')\n",
    "\n",
    "# Check if the data arrays are not empty\n",
    "if X_train.size == 0 or Y_train.size == 0 or X_valid.size == 0 or Y_valid.size == 0:\n",
    "    raise ValueError(\"Expected input data to be non-empty.\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=32, return_sequences=True, input_shape=(n_lookback, X_train.shape[2])))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dense(n_forecast))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=128, verbose=1, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "Y_valid_pred = model.predict(X_valid)\n",
    "\n",
    "# Reshape Y_valid and Y_valid_pred for inverse transform\n",
    "Y_valid_pred_reshaped = Y_valid_pred.reshape(-1, 1)\n",
    "Y_valid_reshaped = Y_valid.reshape(-1, 1)\n",
    "\n",
    "Y_valid_pred_inverse = scalers[target_column].inverse_transform(Y_valid_pred_reshaped).reshape(Y_valid_pred.shape)\n",
    "Y_valid_actual_inverse = scalers[target_column].inverse_transform(Y_valid_reshaped).reshape(Y_valid.shape)\n",
    "\n",
    "validation_rmse = np.sqrt(mean_squared_error(Y_valid_actual_inverse, Y_valid_pred_inverse))\n",
    "print(f'Validation RMSE: {validation_rmse}')\n",
    "\n",
    "# Forecasting using the whole dataset\n",
    "X_ = data[feature_columns].iloc[-n_lookback:].values.reshape(1, n_lookback, len(feature_columns))\n",
    "Y_ = model.predict(X_).reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the forecasted data\n",
    "Y_ = scalers[target_column].inverse_transform(Y_)\n",
    "\n",
    "# Prepare results\n",
    "df_past = data[[target_column]].reset_index()\n",
    "df_past.rename(columns={target_column: 'Actual'}, inplace=True)\n",
    "df_past['Forecast'] = np.nan\n",
    "df_past['Forecast'].iloc[-1] = df_past['Actual'].iloc[-1]\n",
    "\n",
    "df_future = pd.DataFrame(columns=['Date', 'Actual', 'Forecast'])\n",
    "df_future['Date'] = pd.date_range(start=df_past['Date'].iloc[-1] + pd.Timedelta(days=1), periods=n_forecast)\n",
    "df_future['Forecast'] = Y_.flatten()\n",
    "df_future['Actual'] = np.nan\n",
    "\n",
    "results = df_past.append(df_future).set_index('Date')\n",
    "\n",
    "# Print results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 800\n",
      "Validation data length: 200\n",
      "X_train shape: (746, 50, 5)\n",
      "Y_train shape: (746, 5)\n",
      "X_valid shape: (146, 50, 5)\n",
      "Y_valid shape: (146, 5)\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 3s 118ms/step - loss: 0.0630 - val_loss: 0.2671\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0177 - val_loss: 0.0528\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0069 - val_loss: 0.0187\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0033 - val_loss: 0.0153\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0023 - val_loss: 0.0155\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0020 - val_loss: 0.0125\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0013 - val_loss: 0.0106\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0012 - val_loss: 0.0106\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0012 - val_loss: 0.0133\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0119\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0010 - val_loss: 0.0116\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.8910e-04 - val_loss: 0.0127\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 9.6995e-04 - val_loss: 0.0123\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 9.6413e-04 - val_loss: 0.0136\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 9.4264e-04 - val_loss: 0.0120\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 9.2176e-04 - val_loss: 0.0135\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 9.1768e-04 - val_loss: 0.0129\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 9.0226e-04 - val_loss: 0.0130\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 8.9339e-04 - val_loss: 0.0132\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 8.8836e-04 - val_loss: 0.0120\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 8.7883e-04 - val_loss: 0.0123\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 8.6529e-04 - val_loss: 0.0129\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 8.6626e-04 - val_loss: 0.0116\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 8.5153e-04 - val_loss: 0.0124\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 8.4589e-04 - val_loss: 0.0115\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 8.2986e-04 - val_loss: 0.0120\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 8.2259e-04 - val_loss: 0.0111\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 8.1514e-04 - val_loss: 0.0110\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 8.0591e-04 - val_loss: 0.0110\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 7.9722e-04 - val_loss: 0.0115\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 7.8631e-04 - val_loss: 0.0104\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 7.8526e-04 - val_loss: 0.0104\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 7.7989e-04 - val_loss: 0.0106\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 7.6852e-04 - val_loss: 0.0103\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 7.6444e-04 - val_loss: 0.0111\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 7.5344e-04 - val_loss: 0.0090\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 7.4221e-04 - val_loss: 0.0105\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 7.5506e-04 - val_loss: 0.0085\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 7.5882e-04 - val_loss: 0.0122\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 7.5450e-04 - val_loss: 0.0086\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 7.1567e-04 - val_loss: 0.0104\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 7.2016e-04 - val_loss: 0.0096\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 7.1580e-04 - val_loss: 0.0085\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 7.4570e-04 - val_loss: 0.0123\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 7.2969e-04 - val_loss: 0.0074\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 7.0917e-04 - val_loss: 0.0113\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 7.0851e-04 - val_loss: 0.0085\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 6.9461e-04 - val_loss: 0.0090\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 6.8231e-04 - val_loss: 0.0096\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 6.7025e-04 - val_loss: 0.0096\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 6.8952e-04 - val_loss: 0.0084\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.7737e-04 - val_loss: 0.0089\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 6.8677e-04 - val_loss: 0.0112\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 6.6678e-04 - val_loss: 0.0090\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.5143e-04 - val_loss: 0.0106\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 6.6265e-04 - val_loss: 0.0106\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.4973e-04 - val_loss: 0.0089\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 6.3304e-04 - val_loss: 0.0095\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 6.2685e-04 - val_loss: 0.0089\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 6.3977e-04 - val_loss: 0.0133\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 7.0384e-04 - val_loss: 0.0085\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.5150e-04 - val_loss: 0.0086\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 6.2516e-04 - val_loss: 0.0090\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.3518e-04 - val_loss: 0.0125\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 6.7229e-04 - val_loss: 0.0073\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.8666e-04 - val_loss: 0.0085\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.7348e-04 - val_loss: 0.0128\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.5038e-04 - val_loss: 0.0068\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 6.4780e-04 - val_loss: 0.0116\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 6.6475e-04 - val_loss: 0.0102\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 6.1284e-04 - val_loss: 0.0072\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.3869e-04 - val_loss: 0.0121\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.2908e-04 - val_loss: 0.0087\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 5.9109e-04 - val_loss: 0.0095\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 5.9782e-04 - val_loss: 0.0105\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 6.2969e-04 - val_loss: 0.0074\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 6.2264e-04 - val_loss: 0.0094\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 6.0983e-04 - val_loss: 0.0103\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 6.2046e-04 - val_loss: 0.0075\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 5.9632e-04 - val_loss: 0.0098\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 5.7753e-04 - val_loss: 0.0079\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 5.8708e-04 - val_loss: 0.0094\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 5.7139e-04 - val_loss: 0.0089\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 5.8365e-04 - val_loss: 0.0088\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 5.6994e-04 - val_loss: 0.0090\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 5.7296e-04 - val_loss: 0.0108\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 5.8520e-04 - val_loss: 0.0070\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 5.8144e-04 - val_loss: 0.0106\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 5.8820e-04 - val_loss: 0.0103\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 5.7222e-04 - val_loss: 0.0081\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 5.5793e-04 - val_loss: 0.0111\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 5.6108e-04 - val_loss: 0.0075\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 5.7014e-04 - val_loss: 0.0117\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 5.7308e-04 - val_loss: 0.0088\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 5.6756e-04 - val_loss: 0.0100\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 5.6654e-04 - val_loss: 0.0095\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 5.4721e-04 - val_loss: 0.0080\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 5.7450e-04 - val_loss: 0.0116\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 6.1712e-04 - val_loss: 0.0113\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 5.6081e-04 - val_loss: 0.0078\n",
      "5/5 [==============================] - 1s 7ms/step\n",
      "Validation RMSE: 105.53197480146142\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "              Actual    Forecast\n",
      "Date                            \n",
      "2018-04-06  0.020154         NaN\n",
      "2018-04-09  0.018539         NaN\n",
      "2018-04-10  0.021058         NaN\n",
      "2018-04-11  0.020427         NaN\n",
      "2018-04-12  0.019280         NaN\n",
      "...              ...         ...\n",
      "2022-03-25       NaN  925.753967\n",
      "2022-03-26       NaN  889.380920\n",
      "2022-03-27       NaN  905.619690\n",
      "2022-03-28       NaN  848.929504\n",
      "2022-03-29       NaN  894.303101\n",
      "\n",
      "[1005 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_1180\\174330883.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_past['Forecast'].iloc[-1] = df_past['Actual'].iloc[-1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Sample data\n",
    "data = pd.read_csv(\"TSLA.csv\")\n",
    "data = data.tail(1000)\n",
    "\n",
    "# Ensure Date is in datetime format and set it as index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Identify target column and feature columns\n",
    "target_column = 'Close'\n",
    "feature_columns = data.columns.tolist()\n",
    "feature_columns.remove(target_column)\n",
    "\n",
    "# Scaling features and target column\n",
    "scalers = {}\n",
    "for column in data.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[[column]] = scaler.fit_transform(data[[column]])\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# Parameters\n",
    "n_lookback = 50  # number of past days to look back\n",
    "n_forecast = 5   # number of days to forecast\n",
    "validation_split = 0.2\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_size = int(len(data) * (1 - validation_split))\n",
    "data_train = data.iloc[:train_size]\n",
    "data_valid = data.iloc[train_size:]\n",
    "\n",
    "print(f'Training data length: {len(data_train)}')\n",
    "print(f'Validation data length: {len(data_valid)}')\n",
    "\n",
    "# Prepare training data\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for i in range(n_lookback, len(data_train) - n_forecast + 1):\n",
    "    X_train.append(data_train[feature_columns].iloc[i - n_lookback: i].values)\n",
    "    Y_train.append(data_train[target_column].iloc[i: i + n_forecast].values)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "\n",
    "# Prepare validation data\n",
    "X_valid = []\n",
    "Y_valid = []\n",
    "\n",
    "for i in range(n_lookback, len(data_valid) - n_forecast + 1):\n",
    "    X_valid.append(data_valid[feature_columns].iloc[i - n_lookback: i].values)\n",
    "    Y_valid.append(data_valid[target_column].iloc[i: i + n_forecast].values)\n",
    "\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)\n",
    "\n",
    "print(f'X_valid shape: {X_valid.shape}')\n",
    "print(f'Y_valid shape: {Y_valid.shape}')\n",
    "\n",
    "# Check if the data arrays are not empty\n",
    "if X_train.size == 0 or Y_train.size == 0 or X_valid.size == 0 or Y_valid.size == 0:\n",
    "    raise ValueError(\"Expected input data to be non-empty.\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=32, return_sequences=True, input_shape=(n_lookback, X_train.shape[2])))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dense(n_forecast))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=128, verbose=1, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "Y_valid_pred = model.predict(X_valid)\n",
    "\n",
    "# Reshape Y_valid and Y_valid_pred for inverse transform\n",
    "Y_valid_pred_reshaped = Y_valid_pred.reshape(-1, 1)\n",
    "Y_valid_reshaped = Y_valid.reshape(-1, 1)\n",
    "\n",
    "Y_valid_pred_inverse = scalers[target_column].inverse_transform(Y_valid_pred_reshaped).reshape(Y_valid_pred.shape)\n",
    "Y_valid_actual_inverse = scalers[target_column].inverse_transform(Y_valid_reshaped).reshape(Y_valid.shape)\n",
    "\n",
    "validation_rmse = np.sqrt(mean_squared_error(Y_valid_actual_inverse, Y_valid_pred_inverse))\n",
    "print(f'Validation RMSE: {validation_rmse}')\n",
    "\n",
    "# Forecasting using the whole dataset\n",
    "X_ = data[feature_columns].iloc[-n_lookback:].values.reshape(1, n_lookback, len(feature_columns))\n",
    "Y_ = model.predict(X_).reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the forecasted data\n",
    "Y_ = scalers[target_column].inverse_transform(Y_)\n",
    "\n",
    "# Prepare results\n",
    "df_past = data[[target_column]].reset_index()\n",
    "df_past.rename(columns={target_column: 'Actual'}, inplace=True)\n",
    "df_past['Forecast'] = np.nan\n",
    "df_past['Forecast'].iloc[-1] = df_past['Actual'].iloc[-1]\n",
    "\n",
    "df_future = pd.DataFrame(columns=['Date', 'Actual', 'Forecast'])\n",
    "df_future['Date'] = pd.date_range(start=df_past['Date'].iloc[-1] + pd.Timedelta(days=1), periods=n_forecast)\n",
    "df_future['Forecast'] = Y_.flatten()\n",
    "df_future['Actual'] = np.nan\n",
    "\n",
    "results = pd.concat([df_past, df_future]).set_index('Date')\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2018-04-06    0.020154\n",
      "2018-04-09    0.018539\n",
      "2018-04-10    0.021058\n",
      "2018-04-11    0.020427\n",
      "2018-04-12    0.019280\n",
      "                ...   \n",
      "2022-03-18    0.728234\n",
      "2022-03-21    0.741440\n",
      "2022-03-22    0.802423\n",
      "2022-03-23    0.806719\n",
      "2022-03-24    0.819121\n",
      "Name: Close, Length: 1000, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-06</td>\n",
       "      <td>0.020154</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>0.018539</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-10</td>\n",
       "      <td>0.021058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>0.020427</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>0.019280</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>0.672042</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>0.643812</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>2022-03-08</td>\n",
       "      <td>0.660410</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>0.689360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>2022-03-10</td>\n",
       "      <td>0.672050</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date    Actual  Forecast\n",
       "0   2018-04-06  0.020154       NaN\n",
       "1   2018-04-09  0.018539       NaN\n",
       "2   2018-04-10  0.021058       NaN\n",
       "3   2018-04-11  0.020427       NaN\n",
       "4   2018-04-12  0.019280       NaN\n",
       "..         ...       ...       ...\n",
       "985 2022-03-04  0.672042       NaN\n",
       "986 2022-03-07  0.643812       NaN\n",
       "987 2022-03-08  0.660410       NaN\n",
       "988 2022-03-09  0.689360       NaN\n",
       "989 2022-03-10  0.672050       NaN\n",
       "\n",
       "[990 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['Close'].scale)\n",
    "\n",
    "df_past.head(-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, Actual, Forecast]\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_future.head(-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
